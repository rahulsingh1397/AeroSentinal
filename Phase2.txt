ğŸš€ Aerosentinel â€” Next Phases Roadmap (Step-by-Step)
Phase 2: Intelligence Expansion (Perception â†’ Tracking)

Goal: Move from â€œseeing objectsâ€ to â€œunderstanding motionâ€

Step 2.1 â€” Upgrade Vision Stack

Replace YOLOv8-n with YOLOv8-m
Enable CUDA + TensorRT inference
Target: â‰¥60 FPS at 1080p on RTX 5060 Ti

Outcome:
More stable detections, fewer false positives, better distance consistency

Step 2.2 â€” Add Multi-Object Tracking (MOT)

Integrate DeepSORT
Assign persistent object IDs
Track velocity vectors per object

Outcome:
Objects are no longer â€œframe-by-frameâ€ â€” they have identity and memory

Step 2.3 â€” Smart Track v2

Allow operator to:
Select object class (person / vehicle)
Lock onto a specific ID
Implement loss-of-target recovery logic

Outcome:
Reliable cinematic tracking without manual re-selection

Phase 3: Spatial Awareness (Seeing â†’ Understanding Space)
Goal: Add depth & environmental context

Step 3.1 â€” Depth Estimation
Integrate MiDaS (small model)
Generate relative depth maps from RGB feed
Fuse depth with bounding boxes

Outcome:
Distance-aware tracking (near vs far objects)

Step 3.2 â€” Proximity Awareness

Combine:
Depth estimation
DJI obstacle telemetry
Generate collision risk score

Outcome:
Visual + audio warnings before unsafe maneuvers

Step 3.3 â€” Camera Framing Intelligence

Auto-adjust:
Gimbal pitch
Zoom framing
Maintain rule-of-thirds composition

Outcome:
Cinematography feels intentional, not reactive

Phase 4: Autonomy Layer (Assisted â†’ Semi-Autonomous)
Goal: Let AI assist â€” not replace â€” the pilot

Step 4.1 â€” Mission Intelligence Engine

AI suggests:

Orbit radius
Tracking speed
Altitude band
Operator retains final control

Outcome:
AI becomes a copilot, not an autopilot

Step 4.2 â€” AI Shot Selection

Automatically choose:
Orbit
Dronie
Follow

Based on:
Subject motion
Environment openness

Outcome:
One-click cinematic shots powered by AI context

Step 4.3 â€” Safety Envelope Enforcement

Hard limits:
Max pitch
Max yaw rate
Min altitude
Override AI if constraints violated

Outcome:
Regulatory-safe autonomy (no reckless AI behavior)

Phase 5: Edge Intelligence (Latency â†’ Resilience)
Goal: Reduce dependence on PC inference

Step 5.1 â€” On-Device AI (Android)
Deploy YOLOv8-nano (INT8) on phone
Run low-FPS fallback detection

Outcome:
System remains functional if PC link drops

Step 5.2 â€” Hybrid Inference Mode
PC = high-accuracy vision
Phone = emergency perception
Automatic failover logic

Outcome:
Mission continuity under degraded conditions

Phase 6: AI Operator Interface (Controls â†’ Commands)
Goal: Natural humanâ€“AI interaction

Step 6.1 â€” Voice Command Engine

Local LLM (7B, 4-bit)
Commands like:
â€œTrack the cyclistâ€
â€œStart orbit shotâ€

Outcome:
Hands-free AI control without cloud dependency

Step 6.2 â€” Natural Language Mission Builder
Convert text â†’ waypoint logic
Validate commands before execution

Outcome:
Complex missions created in seconds, safely

Phase 7: Learning System (Static â†’ Adaptive)
Goal: Build a data flywheel

Step 7.1 â€” Flight Data Logging

Store:
Frames
Detections
Operator overrides

Outcome:
High-quality labeled dataset

Step 7.2 â€” Model Fine-Tuning

Periodic YOLO fine-tuning

Improve:

Tracking stability

Domain-specific accuracy

Outcome:
System improves over time â€” without risky self-learning

ğŸ¯ Final Positioning Statement

Aerosentinel evolves from a real-time AI perception system into a semi-autonomous,
safety-aware, cinematic flight copilot â€” optimized for consumer-grade GPUs like the
RTX 5060 Ti.